{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67-M1nEeS0Mx"
   },
   "source": [
    "# Read in cvs of tweets and get GPT analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2NQm37JYczV"
   },
   "source": [
    "### Run all cells:\n",
    "  - `ctrl`+`shift`+`P` -> \"run all cells in notebook\"\n",
    "\n",
    "    Or\n",
    "  - Runtime > Run all\n",
    "\n",
    "    Or\n",
    "  - `ctrl` + `F9`\n",
    "### Once complete, final excel files will be saved to the local runtime environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMzA6nVwj8GB"
   },
   "source": [
    "# Read Scraped Tweets and Do Some Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK1bSrPZHfQh"
   },
   "source": [
    "## Download the CSV from the scraped epidural tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLrvXUy8Azks",
    "outputId": "f5bfa634-7796-450e-eb46-35a18915f9aa"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://media.githubusercontent.com/media/kswanjitsu/epidural/main/filtered_df.csv'\n",
    "resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzS3YtSLHx45"
   },
   "source": [
    "### Read data to pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "bxlLXZcUBp6_",
    "outputId": "34014e6f-494f-42bf-d9dd-6a48aba0f448"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(io.StringIO(resp.text))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p9JA3bh5n-m"
   },
   "source": [
    "## Remove Dupes and Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "65LgMw9_y21D",
    "outputId": "e63cc8e5-79a8-422c-a70f-18aeebc9bd43"
   },
   "outputs": [],
   "source": [
    "df_no_dupes = df.drop_duplicates(subset=['cleaned_tweet'])\n",
    "df_no_dupes_or_rts = df_no_dupes[~df_no_dupes['cleaned_tweet'].str.startswith('rt @')]\n",
    "df = df_no_dupes_or_rts\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Oqk4hlgj1XV"
   },
   "source": [
    "# GPT Analysis Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXroOcvo5zJ9"
   },
   "source": [
    "## Count tokens per tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vp9UIM7Nz-TC",
    "outputId": "51f0b3ae-1508-4988-b8cc-fff9129d8d46"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 944
    },
    "id": "Z9ir4N3gy8CX",
    "outputId": "2ce5e508-7b0f-4c93-afa0-e8c68cc89911"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "# gpt_model = 'gpt-3.5-turbo'\n",
    "gpt_model = 'gpt-4'\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "df['tok_per_twt'] = df['cleaned_tweet'].apply(lambda x: num_tokens_from_string(x, tiktoken.encoding_for_model(gpt_model).name))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OueuwUVzH-L5"
   },
   "source": [
    "### Install OpenAI and Langchain libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpzXMlMzEwl1",
    "outputId": "945d023f-099d-442a-f6e7-649cf8dccc8c"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade openai langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hxm2KJGHIM_p"
   },
   "source": [
    "## Setup OpenAI key and instatiate a chat model to make the API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qscqeHBLCImO"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key  = '<your_open_ai_key>'\n",
    "os.environ['OPENAI_API_KEY'] = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lqxRG4kEzT1",
    "outputId": "0f6b15c6-6985-4f31-ce02-688e92cf885a"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0, model_name=gpt_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xaHHm79IYu5"
   },
   "source": [
    "### Prompt template\n",
    "\n",
    "Give the model instructions to interpret the message and format a response.\n",
    "\n",
    "**_text_** is a list of tweets string built in subsequent cell\n",
    "\n",
    "**_format_instructions_** is a langchain output parser, also built in subsequent cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Il97izg0HaR7"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = '''There is a list of posts demarcated by triple backticks (```).\n",
    "\n",
    "For each item in the list, determine:\n",
    "(1) if the author had an epidural or not\n",
    "(2) if their opinion of epidurals is positive, negative, or neutral\n",
    "(3) if their opinion on natural childbirth is positive, negative, or neutral\n",
    "\n",
    "```{text}```\n",
    "\n",
    "Respond with a list of python dictionaries corresponding to each message in the list.\n",
    "{format_instructions}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKulNt-wVOxq"
   },
   "source": [
    "#### Format instructions:\n",
    "- We want the model to return a list of json objects\n",
    "- This code describes each of the fields in the those JSON objects. This way we can parse the responses and convert it more easily into usable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_8ofLt9HBKd"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "\n",
    "index_schema = ResponseSchema(name=\"index\",\n",
    "                            description=\"The index number in front of the list entry\")\n",
    "\n",
    "about_epi_schema = ResponseSchema(name=\"about_epi\",\n",
    "                            description=\"Is the author mainly talking about epidurals or natural?\\\n",
    "                            1 for 'epidurals', 0 for 'natural', NaN if you could not determine.\\\n",
    "                            Response should ONLY be one of: [1, 0, NaN]\")\n",
    "had_epi_schema = ResponseSchema(name=\"had_epi\",\n",
    "                            description=\"Did the author have an epidural?\\\n",
    "                            1 for 'yes', 0 for 'no', NaN if you could not determine.\\\n",
    "                            Response should ONLY be one of: [1, 0, NaN]\")\n",
    "epi_pos_schema = ResponseSchema(name=\"epi_pos\",\n",
    "                            description=\"Does the message display a positive sentiment towards epidurals?\\\n",
    "                            1 for 'yes', 0 for 'neutral', -1 for 'negative sentiment', NaN if you could not determine.\\\n",
    "                            Response should ONLY be one of: [1, 0, -1, NaN]\")\n",
    "\n",
    "nat_pos_schema = ResponseSchema(name=\"nat_pos\",\n",
    "                            description=\"Does the message display a positive sentiment towards natural births?\\\n",
    "                            1 for 'yes', 0 for 'neutral', -1 for 'negative sentiment', NaN if you could not determine.\\\n",
    "                            Response should ONLY be one of: [1, 0, -1, NaN]\")\n",
    "\n",
    "response_schemas = [index_schema, about_epi_schema, had_epi_schema, epi_pos_schema, nat_pos_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = ChatPromptTemplate.from_template(template=template_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v7382iAI9Kp"
   },
   "source": [
    "## Sample the full dataset and remove no-longer-needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "id": "zxPeIrrPSXs3",
    "outputId": "215700e4-8535-435f-ce69-5e73f01edcc4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples = 40000\n",
    "sample_of_tweets_df = df.sample(n=num_samples)\n",
    "\n",
    "############## Only keep the cleaned_tweets for this section ############\n",
    "sample_of_tweets_df = sample_of_tweets_df[['cleaned_tweet']]\n",
    "# Save off the old index so it will be easy to merge the results DF and the original\n",
    "sample_of_tweets_df['old_index'] = sample_of_tweets_df.index\n",
    "\n",
    "# Reset the index so it will be easier to group the tweets and send them as batches\n",
    "sample_of_tweets_df.reset_index(inplace=True)\n",
    "sample_of_tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUhtYfvWSXZ2"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImzRzAycV1bi"
   },
   "source": [
    "## Send the tweets to the model\n",
    "\n",
    "### The groupby snags x rows from `sample_of_tweets_df` at a time, then builds a list of the tweets preceeded by their original index. The format instructions are already embedded into the prompt. Here we are creating the `text` component.\n",
    "- For each batch, get and parse the responses. The response dictionaries are appended to a list of response dictionaries.\n",
    "- Lots of debugging prints commented out. Feel free to uncomment to see what's going on, but it may add significant amounts of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dC91A56Ob0l",
    "outputId": "cb22195c-4b44-4940-e985-6a26b12ab0f7"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import regex as re\n",
    "\n",
    "\n",
    "all_responses_list_of_dicts = list()\n",
    "\n",
    "send_message = False\n",
    "all_messages = []\n",
    "\n",
    "\n",
    "group_size = 20\n",
    "\n",
    "for i, g in sample_of_tweets_df.groupby(sample_of_tweets_df.index // group_size):\n",
    "  try:\n",
    "    # print('-'*50)\n",
    "    print(f'\\rGroup #{i}', end='')\n",
    "    msg_list = []\n",
    "    # print(g.to_dict())\n",
    "    for j, data in g.iterrows():\n",
    "      old_index = data['old_index']\n",
    "      tweet = data['cleaned_tweet']\n",
    "      # print(f'Adding tweet {old_index}: {tweet}')\n",
    "      msg_list.append(f'{old_index}) {tweet.encode(\"UTF-8\")}')\n",
    "    messages = prompt.format_messages(text='\\n'.join(msg_list),\n",
    "                              format_instructions=format_instructions)\n",
    "    all_messages.append(messages[0].content)\n",
    "    # print(messages[0].content)\n",
    "    if send_message:\n",
    "      response = chat(messages)\n",
    "      m = re.findall(r'\\{([^{]*?)\\}', response.content)\n",
    "      all_responses_list_of_dicts.extend([json.loads(f'{{{item}}}') for item in m])\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# print('='*50)\n",
    "# print(all_responses_list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhvZHXbANVld",
    "outputId": "15e2720f-d30c-40a4-a61a-3e5bb7397139"
   },
   "outputs": [],
   "source": [
    "# Sample of the responses\n",
    "print(len(all_responses_list_of_dicts))\n",
    "print('Sample...:')\n",
    "all_responses_list_of_dicts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkHff3_nWlIo"
   },
   "source": [
    "### Put the list of response dicitonaries into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "UP6cHY07TPZB",
    "outputId": "d1fefd53-e1fd-47b2-a3df-81aa76bfb5ed"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "temp_df = pd.DataFrame(all_responses_list_of_dicts)\n",
    "temp_df['index'] = pd.to_numeric(temp_df['index'])\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce_859bzWq3C"
   },
   "source": [
    "## Merge the results with the original `sample_of_tweets_df` and original full dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "5KvZpQRT5gpj",
    "outputId": "16b4ca50-442b-4b87-f6c9-579a500b2a87"
   },
   "outputs": [],
   "source": [
    "merged_results = sample_of_tweets_df.merge(right=temp_df, how='inner', left_on='old_index', right_on='index')\n",
    "merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "dNjfT5V0ToEc",
    "outputId": "5fa10496-18e5-4a42-b000-77fb063b533b"
   },
   "outputs": [],
   "source": [
    "df['old_index'] = df.index\n",
    "filtered_df_merged_results = df.merge(right=temp_df, how='inner', left_on='old_index', right_on='index')\n",
    "filtered_df_merged_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0CxSY7UXV5P"
   },
   "source": [
    "## Save and Download results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wwbt01TUkK_"
   },
   "outputs": [],
   "source": [
    "merged_results_file = 'merged_results.xlsx'\n",
    "filtered_df_merged_results_file = 'filtered_df_merged_results.xlsx'\n",
    "\n",
    "merged_results.to_excel(merged_results_file)\n",
    "filtered_df_merged_results.to_excel(filtered_df_merged_results_file)\n",
    "sample_of_tweets_df.to_excel('sample_of_tweets_df.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Koi993plkXAG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "cMzA6nVwj8GB"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
